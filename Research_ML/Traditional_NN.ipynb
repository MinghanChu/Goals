{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c0d149",
   "metadata": {},
   "source": [
    "# Traditional Neural Network\n",
    "\n",
    "In a neural network (NN), **transformation** refers to the process where an **input is passed through multiple layers**, undergoing a series of operations (like **linear transformations and nonlinear activations**) to produce an output. This process is known as **forward propagation**.\n",
    "\n",
    "How a Traditional NN Learns from Data Using Backpropagation\n",
    "\n",
    "A neural network learns by adjusting its internal weights to minimize the difference between predicted and true values. This process consists of three key steps:\n",
    "\n",
    "1. Forward Propagation (Transformation)\n",
    "- The input  x  is passed through the network layer by layer.\n",
    "- Each layer applies a transformation to the input:\n",
    "\n",
    "$$x_{l+1} = f(W_l x_l + b_l)$$\n",
    "\n",
    "where:\n",
    "- $W_l$  = weight matrix at layer  l \n",
    "- $b_l$  = bias at layer  l \n",
    "- $f(\\cdot)$  = activation function (e.g., ReLU, sigmoid)\n",
    "- $x_l$  = output of previous layer\n",
    "- The final layer produces the output  $\\hat{y}$ , which is the networkâ€™s prediction.\n",
    "\n",
    "2. Compute Loss (Error Calculation)\n",
    "- The difference between the predicted output  $\\hat{y}$  and the actual label  $y$  is measured using a loss function  $\\mathcal{L}(\\hat{y}, y)$.\n",
    "\n",
    "Example loss functions:\n",
    "- Mean Squared Error (MSE) for regression:\n",
    "\n",
    "$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- Cross-Entropy Loss for classification:\n",
    "\n",
    "$\\mathcal{L} = - \\sum_{i} y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "\n",
    "3. Backpropagation (Weight Update)\n",
    "\n",
    "To minimize the loss, we update the weights using backpropagation, which consists of:\n",
    "\n",
    "Step 1: Compute Gradients Using Chain Rule\n",
    "- We compute how much the loss changes with respect to each weight using the chain rule of differentiation:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial W}$$\n",
    "\n",
    "This gives us the gradient, which tells us the direction and magnitude of change needed.\n",
    "\n",
    "Step 2: Update Weights Using Gradient Descent\n",
    "- The gradients are used to update the weights with a small step  \\eta  (learning rate):\n",
    "\n",
    "$$W = W - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}$$\n",
    "\n",
    "- If the gradient is large, the weights change more.\n",
    "- If the gradient is small, the weights change less.\n",
    "- This step is repeated for all weights in all layers.\n",
    "\n",
    "4. Iterate Until Convergence\n",
    "- The forward pass, loss computation, and backpropagation are repeated for many iterations (epochs) until the loss stops decreasing significantly.\n",
    "- The model eventually learns an optimal mapping from input  x  to output  y ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
