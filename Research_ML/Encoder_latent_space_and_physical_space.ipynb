{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a4271f",
   "metadata": {},
   "source": [
    "# An Example\n",
    "\n",
    "Start with an example:\n",
    "\n",
    "Loading the model's state dictionary and inspecting the layers with the **pre-trained model** for case 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d85be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "model_state_dict\n",
      "optim_states_dict\n",
      "optim_net_dec_dict\n",
      "scheduler_states_dict\n",
      "scheduler_net_dec_dict\n",
      "hidden_states_model\n",
      "loss\n",
      "log_epoches\n",
      "eval_losses\n",
      "psnrs\n",
      "Model layers: odict_keys(['net1.0.weight', 'net1.0.bias', 'net1.1.weight', 'net1.1.bias', 'net1.2.weight', 'net1.2.bias', 'net1.3.weight', 'net1.3.bias', 'net1.4.weight', 'net1.4.bias', 'net1.5.weight', 'net1.5.bias', 'net1.6.weight', 'net1.6.bias', 'net1.7.weight', 'net1.7.bias', 'net1.8.weight', 'net1.8.bias', 'net1.9.weight', 'net1.9.bias', 'net1.10.weight', 'net1.10.bias', 'net1.11.weight', 'net1.11.bias', 'net1.12.weight', 'net1.12.bias', 'net1.13.weight', 'net1.13.bias', 'net1.14.weight', 'net1.14.bias', 'net1.15.weight', 'net1.15.bias', 'net1.16.weight', 'net1.16.bias', 'net2.0.weight', 'net2.1.weight', 'net2.2.weight', 'net2.3.weight', 'net2.4.weight', 'net2.5.weight', 'net2.6.weight', 'net2.7.weight', 'net2.8.weight', 'net2.9.weight', 'net2.10.weight', 'net2.11.weight', 'net2.12.weight', 'net2.13.weight', 'net2.14.weight', 'net2.15.weight'])\n",
      "Layer: net1.0.weight, Shape: torch.Size([384, 3])\n",
      "Layer: net1.0.bias, Shape: torch.Size([384])\n",
      "Layer: net1.1.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.1.bias, Shape: torch.Size([384])\n",
      "Layer: net1.2.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.2.bias, Shape: torch.Size([384])\n",
      "Layer: net1.3.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.3.bias, Shape: torch.Size([384])\n",
      "Layer: net1.4.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.4.bias, Shape: torch.Size([384])\n",
      "Layer: net1.5.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.5.bias, Shape: torch.Size([384])\n",
      "Layer: net1.6.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.6.bias, Shape: torch.Size([384])\n",
      "Layer: net1.7.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.7.bias, Shape: torch.Size([384])\n",
      "Layer: net1.8.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.8.bias, Shape: torch.Size([384])\n",
      "Layer: net1.9.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.9.bias, Shape: torch.Size([384])\n",
      "Layer: net1.10.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.10.bias, Shape: torch.Size([384])\n",
      "Layer: net1.11.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.11.bias, Shape: torch.Size([384])\n",
      "Layer: net1.12.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.12.bias, Shape: torch.Size([384])\n",
      "Layer: net1.13.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.13.bias, Shape: torch.Size([384])\n",
      "Layer: net1.14.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.14.bias, Shape: torch.Size([384])\n",
      "Layer: net1.15.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net1.15.bias, Shape: torch.Size([384])\n",
      "Layer: net1.16.weight, Shape: torch.Size([3, 384])\n",
      "Layer: net1.16.bias, Shape: torch.Size([3])\n",
      "Layer: net2.0.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.1.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.2.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.3.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.4.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.5.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.6.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.7.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.8.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.9.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.10.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.11.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.12.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.13.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.14.weight, Shape: torch.Size([384, 384])\n",
      "Layer: net2.15.weight, Shape: torch.Size([384, 384])\n",
      "Final layer: net2.15.weight, Shape: torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/tkxqx5c57fgcftsfpjb5zk8c0000gn/T/ipykernel_13303/2936427539.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "checkpoint_path = \"/Users/minghan/Library/CloudStorage/GoogleDrive-john.chuicandoit@gmail.com/My Drive/CoNFiLD/ConditionalDiffusionGeneration/inference_scripts/Case4/random_sensor/input/cnf_model/checkpoint_20000.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "for key in checkpoint.keys():\n",
    "    print(key)  # Look for something related to 'channel_mult'\n",
    "\n",
    "# Extract model state dict\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "# Print all layer names\n",
    "print(\"Model layers:\", state_dict.keys())\n",
    "\n",
    "# Look for the final convolution layer (it usually starts with 'out' or 'conv_out')\n",
    "for key, value in state_dict.items():\n",
    "    print(f\"Layer: {key}, Shape: {value.shape}\")\n",
    "\n",
    "# Find the last layer\n",
    "last_layer = list(state_dict.keys())[-1]\n",
    "print(f\"Final layer: {last_layer}, Shape: {state_dict[last_layer].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f59e4",
   "metadata": {},
   "source": [
    "The `torch.Size([384, 384])` in the final layer of your CNF model suggests that the model’s final operation is not directly in **physical space** but instead in **latent space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ce2f0",
   "metadata": {},
   "source": [
    "## Physical Space vs Latent Space\n",
    "\n",
    "The reason why `[384, 384]` appears in latent space rather than `[xx, xx, xx, xx]` (which is typical for physical space) is because the structure of data changes when **transitioning from physical space to latent space**.\n",
    "\n",
    "| **Space**               | **Typical Shape**                                              | **What it represents?**                                       |\n",
    "|--------------------------|----------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Physical Space (CFD Grid)**              |`[Nx, Ny, Nz, Channels]` (e.g., `[384, 384, 3]`)           | Velocity, pressure, etc. defined on a physical grid    |\n",
    "| **Latent Space**        | `[Feature_Dim1, Feature_Dim2]` (e.g., `[384, 384]`)                | Encoded feature representation, not directly spatial|\n",
    "\n",
    "$\\underline{Physical Space}$\n",
    "\n",
    "In physical space, data is typically structured based on spatial dimensions:\n",
    "- 3D Flow Example -> `[Nx, Ny, Nz, Channels]`\n",
    "- `Nx, Ny, Nz` = grid resolution in the `x, y, z` directions.\n",
    "- Channels = flow quantities (e.g., velocity components  `u, v, w`, pressure  `p` ).\n",
    "- Example: `[384, 384, 3]` could mean a 2D flow with 3 velocity components (`u, v, w`) at each point.\n",
    "\n",
    "2D Flow Example -> `[Nx, Ny, Channels]`\n",
    "- Example: `[384, 384, 2]` means a `2D` periodic hill case, where only  `u, v`  are considered.\n",
    "\n",
    "**Physical space is explicitly tied to spatial locations.**\n",
    "\n",
    "$\\underline{\\textbf{Latent Space}}$\n",
    "\n",
    "When transitioning to **latent space**, we move from **explicit spatial values** to **compressed feature representations**:\n",
    "\n",
    "- The numbers `384 × 384` in latent space no longer refer to physical grid points.\n",
    "- Instead, they represent **abstract features** that describe the **flow statistically**.\n",
    "- The model learns to **encode complex flow interactions into these feature vectors**.\n",
    "\n",
    "Although the same shape in the latent space, e.g., `[384, 384]`, as that in the physical space is used for this specific case,  \n",
    "- The specific size depends on how the CNF encoder structures the data.\n",
    "- Some latent spaces compress more aggressively (e.g., [256, 256]).\n",
    "- Some keep similar dimensions but store different types of information (e.g., [384, 384] is not a spatial map but **a learned representation of flow dynamics**).\n",
    "\n",
    "**Latent space is not tied to physical locations but instead captures compressed relationships between flow features.**\n",
    "\n",
    "$\\underline{\\textbf{In short}}$\n",
    "\n",
    "- Physical space follows grid-based discretization -> `[Nx, Ny, Nz, Channels]`\n",
    "- Latent space follows abstract feature encoding -> `[Feature_Dim1, Feature_Dim2]`\n",
    "- The **diffusion model learns in latent space**, then decodes back to **physical space**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8931d3",
   "metadata": {},
   "source": [
    "## Encoder (CNF)\n",
    "\n",
    "The **Conditional Neural Field (CNF)** acts as a **feature extractor** that **optimizes the data** before feeding it into the **diffusion model**. This helps the diffusion model focus on learning the most **essential flow patterns** rather than being overwhelmed by **redundant information**.\n",
    "\n",
    "Encoder like CNF is important for diffusion models because:\n",
    "\n",
    "1. Filters out redundancy:\n",
    "- The raw DNS flow fields contain tons of correlated data points that don’t all contribute independently to the physics.\n",
    "- The CNF compresses the input so that only the **most relevant turbulence structures remain**.\n",
    "\n",
    "2. Reduces dimensionality (compression):\n",
    "- Instead of storing every single grid point and every minor fluctuation, the CNF encodes the global flow structure in a more compact way.\n",
    "- This prevents the diffusion model from learning unnecessary details that don’t generalize well.\n",
    "\n",
    "3. Extracts important patterns:\n",
    "- Instead of dealing with raw velocity values per grid point, the CNF finds latent features that summarize the flow structure.\n",
    "- These latent features are **much easier for the diffusion model to process than raw CFD data**.\n",
    "\n",
    "4. Speeds up learning:\n",
    "- Without encoding, the diffusion model would take much longer to converge because it would need to learn both the **essential structures and the redundant noise**.\n",
    "- With CNF, the model already gets **“pre-processed” flow structures**, making training much more efficient.\n",
    "\n",
    "$\\underline{\\textbf{Impact of choice of different Encoder}}$\n",
    "\n",
    "The **choice of encoder is very important because different encoders extract different types of information from the raw data.**\n",
    "\n",
    "1. Good Encoder (like CNF) -> Captures the correct flow features\n",
    "- If the encoder is well-designed, it extracts **meaningful flow structures** that help the diffusion model focus on learning useful patterns.\n",
    "- Example: CNF is trained to preserve the **spatial-temporal coherence of flow features**, which is critical for turbulence modeling.\n",
    "\n",
    "2. Bad Encoder (Poorly Chosen) -> Might **lose important physics**\n",
    "- If an encoder **removes too much information** or **doesn’t retain the right flow structures**, the diffusion model might miss important physics.\n",
    "- Example: If an encoder **is too aggressive in compression**, it might **discard small-scale vortices** that are important in turbulence modeling.\n",
    "\n",
    "3. Different Encoders Extract Different Features\n",
    "- Some encoders might focus more on **spatial structure** (good for smooth flows).\n",
    "- Others might focus more on **temporal evolution** (better for **unsteady turbulence**).\n",
    "- **The best choice depends on what kind of flow you are modeling**.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Type of Encoder | How it works | Good for flow fields |\n",
    "|:--------:|:--------:|:--------:|\n",
    "|  CNF (Neural Field Encoder)   |  Uses coordinate-based representations to encode continuous flow features   |  Yes, because it preserves physics & allows flexible reconstructions |\n",
    "| Autoencoder (AE)   | Compresses spatial data into a bottleneck latent space  |  Sometimes, but might lose fine turbulence details  |\n",
    "|  Fourier-based Encoders   | Uses spectral methods to capture periodic structures  |    Yes, if dealing with periodic turbulence (like Kolmogorov turbulence)  |\n",
    "|  CNN (Convolutional Neural Network) Encoder  |  Extracts spatial patterns using convolution filters |  Yes, but struggles with irregular geometries  |\n",
    "|  PCA (Principal Component Analysis) Encoder   |  Uses linear transformations to reduce dimensionality  |   Too simple for complex turbulence  |\n",
    "\n",
    "**Different encoders extract different features, and the choice of encoder impacts what the diffusion model learns.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
